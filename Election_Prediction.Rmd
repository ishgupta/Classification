---
title: "Election Prediction"
output:
  html_document: default
  html_notebook: default
---

### Load required libraries
```{r, message=FALSE}
library(dplyr)
library(caTools)
library(mice)
```


### Read required data
```{r}
polling <- read.csv("data/PollingData.csv")
glimpse(polling)

table(polling$Year)

summary(polling)
# we have missing values for Rasmussen and SurveyUSA
```

### Handle missing values.
##### Multiple Imputation can be used to impute the missing values
```{r}

# The 4 polling related variables are: "Rasmussen", "SurveyUSA", "PropR", "DiffCount", and if we limit our data to these 4 only.
simple <- polling[c("Rasmussen", "SurveyUSA", "PropR", "DiffCount")]
summary(simple)

set.seed(144)

#Multiple imputation
imputed <- complete(mice(simple))
summary(imputed)

polling$Rasmussen <- imputed$Rasmussen
polling$SurveyUSA <- imputed$SurveyUSA

summary(polling)
```

### Distribute the data into Train and Test sets

```{r}
train <- subset(polling, Year==2004 | Year ==2008)
test <- subset(polling, Year == 2012)

table(train$Republican)

```

#### Understand the prediction of Baseline model against which any Model that we create will be compared. To do that, look at the breakdown of dependent variable in the training set using table()

In 47 of 100 obs., Democrat won the state, and Republican in 53.
Baseline is always going to predict the most common output, so, it will have Republican win the State with the accuracy of 53%.

But it is quite a weak model, since it will alwasy favor Republican and that too for a very weak accuracy.

To find a Smarter Baseline model, which will be to take one of the State observation, and predict the winner.
To compute this, we'll use a sign() which returns 1 if it is passed a +ve number, -1 for -ve and a 0 for Zero.

```{r}
sign(20)

table(sign(train$Rasmussen))

```

Wins- Party
42- Republican
55- Democratic
3- Inconclusive

HOw does the Baseline compare with the actual model.

```{r}
table(train$Republican, sign(train$Rasmussen))


```

This seems to be a much better Model to carry forward against which we can compare the Logistic Regression based approach.

#### Find out the correlation matrix:

```{r}
cor(train[c("Rasmussen", "SurveyUSA", "PropR", "DiffCount", "Republican")])

```
SurveyUSA ~ Rasmussen -> 0.91

The variables in a model will be the ones most related to dependent variable. 

### Fit a model
```{r}

mod1 <- glm(Republican ~ PropR, data = train, family = binomial)
summary(mod1) # AIC: 19.772

pred1 <- predict(mod1, train, type="response")
table(train$Republican, pred1 >= 0.5)  # the model makes only 4 mistakes similar to the BaseLine.

```


### Try another model
```{r}
mod2 <- glm(Republican ~ SurveyUSA + DiffCount, train, family=binomial)
pred2 <- predict(mod2, train, type="response")

table(train$Republican, pred2 > 0.5)
# a total of 3 mistakes. Better than Baseline model

summary(mod2) #AIC: 18.439  Better than mod1. But, variables do not have that much significance.
```
```{r}
table(test$Republican, sign(test$Rasmussen))

```

18 - Smart baseline predicted correctly for Democratic, and was correct.
21- Smart baseline predicted correctly for Republican, and was correct.
2- Inconclusive.

4- Mistakes


So, this the model we'll compare our final model against.

```{r}
testPred = predict(mod2, newdata = test, type="response")

table(test$Republican, testPred > 0.5)
```

Accuracy Error =1 

```{r}
subset(test, testPred > 0.5 & test$Republican == 0)

```


```{r, messsage = FALSE}
library(ROCR)
ROCR_pred <- prediction(testPred, test$Republican)
ROCR_perf <- performance(ROCR_pred, "tpr", "fpr")

plot(ROCR_perf, colorize=TRUE, print.cutoffs.at = seq(0, 1, 0.1), text.adj = c(-0.7, 1.5))
```

